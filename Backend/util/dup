import Groq from "groq-sdk";
import dotenv from "dotenv";
import fs from "fs";
import path from "path";
import { pipeline } from "@xenova/transformers";

dotenv.config();

const groq = new Groq({ apiKey: process.env.GROQ_API_KEY });

// Load app documentation

const APP_DOCS = fs.readFileSync(path.resolve("data/app-docs.txt"), "utf-8");

// Split text into chunks
function splitTextIntoChunks(text, chunkSize = 500) {
  const sentences = text.split(/(?<=[.?!])\s+/);
  const chunks = [];
  let currentChunk = "";

  for (let sentence of sentences) {
    if ((currentChunk + sentence).length > chunkSize) {
      chunks.push(currentChunk.trim());
      currentChunk = sentence + " ";
    } else {
      currentChunk += sentence + " ";
    }
  }
  if (currentChunk) chunks.push(currentChunk.trim());
  return chunks;
}

const DOC_CHUNKS = splitTextIntoChunks(APP_DOCS);

// Initialize embeddings pipeline
let embedder;

async function initEmbedder() {
  embedder = await pipeline("feature-extraction", "Xenova/all-MiniLM-L6-v2");
}

// Generate embedding for text
async function getEmbedding(text) {
  const embeddingTensor = await embedder(text);

  // Convert tensor to plain array
  const array =
    embeddingTensor.data instanceof Float32Array
      ? Array.from(embeddingTensor.data)
      : embeddingTensor.data.flat?.() || [];

  if (array.length === 0) {
    throw new Error("Failed to extract embedding array from tensor");
  }

  return array;
}

// Cosine similarity
function cosineSimilarity(a, b) {
  const dot = a.reduce((sum, val, i) => sum + val * b[i], 0);
  const magA = Math.sqrt(a.reduce((sum, val) => sum + val * val, 0));
  const magB = Math.sqrt(b.reduce((sum, val) => sum + val * val, 0));
  return dot / (magA * magB);
}

// Store embedded docs
let DOC_EMBEDDINGS = [];

async function embedDocumentation() {
  for (const chunk of DOC_CHUNKS) {
    const vector = await getEmbedding(chunk);
    DOC_EMBEDDINGS.push({ text: chunk, vector });
  }
}

// Retrieve top relevant chunks
function retrieveTopChunks(queryVector, topK = 3) {
  return DOC_EMBEDDINGS.map((chunk) => ({
    ...chunk,
    score: cosineSimilarity(queryVector, chunk.vector),
  }))
    .sort((a, b) => b.score - a.score)
    .slice(0, topK);
}

// Main chat function
export async function getChatResponse(conversation = []) {
  try {
    const userMessage = conversation[conversation.length - 1]?.content || "";
    const queryVector = await getEmbedding(userMessage);
    const topChunks = retrieveTopChunks(queryVector);

    const SYSTEM_PROMPT = `
You are a helpful, concise chatbot for the GiftGalore platform.
- Always give short, crisp answers.
- Format responses in Markdown.
- Use:
  - Numbered lists for steps
  - Bulleted lists for options
  - Bold for emphasis
  - Italics for secondary emphasis
  - Tables if necessary
When suggesting products give a link in the format: replace oid from the documentation.
https://giftgalore.netlify.app/$oid
- Do not repeat full sentences from the documentation word-for-word; summarize instead.
- Only answer questions about GiftGaloreâ€™s features, usage, and troubleshooting.
- If unrelated, reply: "Sorry, I can only help with GiftGalore-related questions."
Here is GiftGalore documentation:
${topChunks.map((c) => c.text).join("\n\n")}
`;

    const messages = [
      { role: "system", content: SYSTEM_PROMPT },
      ...conversation,
    ];

    const response = await groq.chat.completions.create({
      model: "llama-3.1-8b-instant",
      messages,
    });

    return response.choices[0].message.content;
  } catch (err) {
    console.error("Groq API error:", err);
    if (err?.error?.code === "rate_limit_exceeded") {
      return "Sorry, your message is too long. Please shorten it.";
    }
    return "Sorry, I couldn't generate a response at this time.";
  }
}

// Initialize at startup
await initEmbedder();
await embedDocumentation();
